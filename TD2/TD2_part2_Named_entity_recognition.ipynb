{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7c0b40",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TD2 part 2: Named entity recognition\n",
    "\n",
    "Dans ce TD, nous allons prendre un datasets où les noms de personnes sont taggés.<br>\n",
    "Nous allons transformer ces données en tenseurs X, y et attention_mask.<br>\n",
    "Nous allons créer un modèle RNN pour prédire si un mot est un nom de personne.<br>\n",
    "Nous allons ensuite créer la loop avec l'optimizer pour apprendre le modèle.<br>\n",
    "Du modèle appris (prédisant sur les tokens), nous allons postprocess les prédictions pour avoir les prédictions sur les noms.\n",
    "\n",
    "Un fois que la loop est créée et que le modèle apprend, nous allons changer la structure du modèle:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n",
    "Lorsqu'on a un bon modèle de prédiction pour les noms de personnes, nous allons l'appliquer à notre projet fil rouge.\n",
    "Utilisez-le tel que. Quelle accuracy ?\n",
    "Ré-entrainez la (les) dernière(s) couche(s) du modèle sur notre jeu de données. A-t-il gagné en accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86402ca3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f552fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "Télécharger le dataset MultiNERD FR [ici](https://github.com/Babelscape/multinerd)<br>\n",
    "Mettez les données dans le dossier data/raw du projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157bc913",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_multinerd_person_words(filename=\"../data/raw/train_fr.tsv\"):\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        tagged_words = [line.strip().split(\"\\t\") for line in f]\n",
    "        \n",
    "        # Joining words until we meet a dot\n",
    "        # Word's label is 1 if 'PER' is in its tag\n",
    "        sentences = []\n",
    "        sentence_labels = []\n",
    "    \n",
    "        this_word = []\n",
    "        this_labels = []\n",
    "        for tagged_word in tagged_words:\n",
    "            if len(tagged_word) < 3:\n",
    "                # not a tagged word\n",
    "                continue\n",
    "            word = tagged_word[1]\n",
    "            tag = tagged_word[2]\n",
    "        \n",
    "            if word == '.':\n",
    "                sentences.append(\" \".join(this_word))\n",
    "                sentence_labels.append(this_labels)\n",
    "            \n",
    "                this_word = []\n",
    "                this_labels = []\n",
    "            else:\n",
    "                this_word.append(word)\n",
    "                this_labels.append(1 * tag.endswith(\"PER\"))\n",
    "\n",
    "    return sentences, sentence_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcba104b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences, labels = extract_multinerd_person_words(\"../data/raw/train_fr.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b09cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "En utilisant le tokenizer d'HuggingFace \"camembert-base\":\n",
    "- Transformer les phrases en tokens\n",
    "- Obtenez des vecteur y qui ont le même nombre d'entrées qu'il y a de tokens dans la phrase\n",
    "- Ayez un tenseur \"attention_mask\" pour savoir sur quels tokens on cherche à predire le label\n",
    "- Transformez les tokens en token_ids (avec le tokenizer)\n",
    "Avec tout cela, vous pouvez former vos tenseurs X, Y et attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8ff1ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7976727231e04fdbbd97fad01487a75d"
      },
      "application/json": {
       "n": 0,
       "total": 445008750,
       "elapsed": 0.007001638412475586,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading model.safetensors",
       "ascii": false,
       "unit": "B",
       "unit_scale": true,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daoud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Daoud\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[   5, 1268, 3572,  323,    9,    6]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1]])\n",
      "Labels: tensor([[0, 1, 1, 1, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, CamembertTokenizer, CamembertForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Exemple de phrase\n",
    "sentence = \"Votre phrase ici.\"\n",
    "\n",
    "# Tokenizer la phrase\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Récupérer les tensor X, Y, et attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Créer un tensor Y (étiquettes initiales, à adapter selon votre structure)\n",
    "# Dans cet exemple, on suppose que chaque mot a une étiquette binaire (0 ou 1)\n",
    "# Vous devrez adapter cela en fonction de la structure réelle de vos étiquettes\n",
    "# Le nombre d'éléments dans Y doit correspondre au nombre de tokens dans la phrase\n",
    "labels = torch.randint(2, (1, input_ids.size(1)))\n",
    "\n",
    "# Afficher les tensors\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# Maintenant, vous pouvez utiliser ces tensors pour entraîner votre modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a3802c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_tokens_and_labels_and_attention_mask(tokenizer, sentence, labels):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    tokens_label = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for word, label in zip(words, labels):\n",
    "        this_tokens = tokenizer.tokenize(word)\n",
    "        tokens += this_tokens\n",
    "        \n",
    "        this_labels = [0] * len(this_tokens)\n",
    "        this_labels[0] = label        \n",
    "        tokens_label += this_labels\n",
    "        \n",
    "        this_attention_mask = [1] + [0] * (len(this_tokens) - 1)\n",
    "        attention_mask += this_attention_mask\n",
    "        \n",
    "    return tokens, tokens_label, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066d1710",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens, label, padding_masks = build_tokens_and_labels_and_attention_mask(tokenizer, sentences[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d73a2b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['▁Il', '▁est', '▁incarné', '▁par', '▁A', 'ustin', '▁S', 'to', 'well']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e62c9b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), 0, tensor(1), 0, 0]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06a1580e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 1, 1, 1, 1, 0, 1, 0, 0]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb94a39",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "Contruisez un modèle RNN comme dans la partie 1. Pour l'instant, il prendra comme arguments:\n",
    "- Vocab size: le nombre de différents tokens du tokenizer (52 000 pour camembert-base)\n",
    "- Embedding dim: la dimension de l'embedding des tokens (par défaut 50)\n",
    "- hidden_dim: la dimension de l'état récurrent de votre RNN (par défaut 20)\n",
    "- tagset_size: la nombre de classes possibles pour les prédictions (ici 2)\n",
    "\n",
    "Dans le forward, votre modèle enchaînera les couches suivantes:\n",
    "- Un embedding\n",
    "- Un RNN\n",
    "- Un ReLU\n",
    "- Une couche linéaire\n",
    "- Un softmax pour que la somme des prédictions pour une entrée soit égale à 1 (la prédiction pour un élément et sa probabilité d'être dans chaque classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e661ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=20, tagset_size=2):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "        # Softmax activation\n",
    "        self.softmax = nn.Softmax(dim=2)  # Softmax along the last dimension (tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # RNN layer\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "\n",
    "        # ReLU activation\n",
    "        relu_output = self.relu(rnn_output)\n",
    "\n",
    "        # Linear layer\n",
    "        linear_output = self.linear(relu_output)\n",
    "\n",
    "        # Softmax activation\n",
    "        output = self.softmax(linear_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Exemple d'utilisation du modèle avec un vocabulaire de taille 52000\n",
    "vocab_size = 52000\n",
    "embedding_dim = 50\n",
    "hidden_dim = 20\n",
    "tagset_size = 2\n",
    "\n",
    "# Instanciation du modèle\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, tagset_size)\n",
    "\n",
    "# Exemple d'entrée (batch de séquences de tokens)\n",
    "example_input = torch.randint(vocab_size, (3, 10))  # Batch size: 3, Sequence length: 10\n",
    "\n",
    "# Passe avant\n",
    "output = model(example_input)\n",
    "\n",
    "# Affichage de la sortie\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04004a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "Je fournis ici une fonction prenant un modèle, des tenseurs X, y, attention_mask.\n",
    "Pour chaque batch:\n",
    "- La loop utilise le modèle pour prédire sur x_batch\n",
    "- Avec attention_mask, elle identifie sur quels tokens les prédictions compte\n",
    "- Elle regarde la cross entropy entre y\\[attention_ids\\] et yhat\\[attention_ids\\]\n",
    "- Elle output un dictionnaire avec le model et la loss au fur et à mesure des itérations\n",
    "\n",
    "Entraînez le modèle avec vos données. <br>\n",
    "Plottez la loss history.<br>\n",
    "Itérez sur le modèle pour l'améliorer:\n",
    "- Changer learning rate. Comment se comporte le modèle\n",
    "- Ajouter des couches denses, ReLU, dropout, normalization\n",
    "- Changer le nombre de layers du RNN, LSTM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b9833f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, X, y, attention_masks, n_epochs=100, lr=0.05, batch_size=128):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    loss_history = []\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y, attention_masks)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (x_batch, y_batch, mask) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            ids = mask.reshape(-1)\n",
    "            yhat = model(x_batch).reshape((-1, 2))[ids]\n",
    "            this_y = y_batch.reshape(-1)[ids]\n",
    "            \n",
    "            loss = loss_function(yhat, this_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            loss_history.append(loss.clone().detach())\n",
    "        \n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Got loss at {epoch}\", np.mean(loss_history[-10:]))\n",
    "    \n",
    "    return {\"model\": model, \"loss_history\": loss_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cf63f72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([140436, 350])\n",
      "y shape: torch.Size([1, 6])\n",
      "Attention Masks shape: torch.Size([140436, 350])\n",
      "Adjusted y shape: torch.Size([350])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daoud\\AppData\\Local\\Temp\\ipykernel_20212\\2713239486.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(labels)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [24]\u001B[0m, in \u001B[0;36m<cell line: 29>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdjusted y shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, y\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Entraîner le modèle avec la fonction train_model\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Récupérer le modèle entraîné et l'historique de la perte\u001B[39;00m\n\u001B[0;32m     32\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "Input \u001B[1;32mIn [23]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, X, y, attention_masks, n_epochs, lr, batch_size)\u001B[0m\n\u001B[0;32m      2\u001B[0m loss_function \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      3\u001B[0m loss_history \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensorDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_masks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n",
      "File \u001B[1;32mc:\\users\\daoud\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py:204\u001B[0m, in \u001B[0;36mTensorDataset.__init__\u001B[1;34m(self, *tensors)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 204\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "\u001B[1;31mAssertionError\u001B[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "# Supposons que vous ayez déjà défini le modèle (SimpleRNN), les données (X, y, attention_masks) et le tokenizer\n",
    "# Instanciation du modèle\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, tagset_size)\n",
    "\n",
    "# Tokenizer vos données\n",
    "tokenized_data = tokenizer(sentences, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Récupérer les tensors X, y, et attention_mask à partir du tokenized_data\n",
    "X = tokenized_data[\"input_ids\"]\n",
    "y = torch.tensor(labels)\n",
    "attention_masks = tokenized_data[\"attention_mask\"]\n",
    "\n",
    "# Afficher les dimensions des tensors\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Attention Masks shape:\", attention_masks.shape)\n",
    "\n",
    "# Ajuster les dimensions de y\n",
    "max_sequence_length = 350\n",
    "y = F.pad(y, pad=(0, max_sequence_length - y.size(1)), value=0)\n",
    "# Ajuster les dimensions de y pour correspondre à X et attention_masks\n",
    "y = y.squeeze(dim=0)\n",
    "\n",
    "# Afficher les dimensions après ajustement\n",
    "print(\"Adjusted y shape:\", y.shape)\n",
    "\n",
    "\n",
    "# Entraîner le modèle avec la fonction train_model\n",
    "result = train_model(model, X, y, attention_masks, n_epochs=100, lr=0.05, batch_size=128)\n",
    "\n",
    "# Récupérer le modèle entraîné et l'historique de la perte\n",
    "trained_model = result[\"model\"]\n",
    "loss_history = result[\"loss_history\"]\n",
    "\n",
    "# Plotter l'historique de la perte\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.show()\n",
    "\n",
    "# Itérer sur le modèle pour l'améliorer\n",
    "# Par exemple, ajustez le learning rate, ajoutez des couches, etc.\n",
    "\n",
    "# Exemple d'ajustement du learning rate et poursuite de l'entraînement\n",
    "result = train_model(trained_model, X, y, attention_masks, n_epochs=50, lr=0.01, batch_size=128)\n",
    "\n",
    "# Récupérer le modèle mis à jour et l'historique de la perte\n",
    "updated_model = result[\"model\"]\n",
    "updated_loss_history = result[\"loss_history\"]\n",
    "\n",
    "# Plotter l'historique de la perte mis à jour\n",
    "plt.plot(updated_loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Updated Training Loss History')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a6933",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Postprocessing\n",
    "\n",
    "Créer une fonction prenant les prédictions du modèle (au niveau token) et sort les prédictions au niveau mot.<br>\n",
    "Par exemple, admettons que, pour un mot, la prédiction du 1er token est la seule qu'on considère.<br>\n",
    "si la phrase est \"Bonjour John\", avec les tokens \\[\"bon\", \"jour\", \"Jo\", \"hn\"\\] avec les predictions \\[0.12, 0.65, 0.88, 0.45\\]<br>\n",
    "Je veux récupérer les prédictions \"bonjour\": 0.12, \"John\": 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def aggregate_predictions_by_word(token_predictions, words):\n",
    "    word_predictions = {}\n",
    "    current_word = \"\"\n",
    "    current_sum = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for token_pred, word in zip(token_predictions, words):\n",
    "        if word.startswith(\"##\"):\n",
    "            # Traitement des tokens qui font partie d'un mot (suite du mot)\n",
    "            current_word += word[2:]\n",
    "            current_sum += token_pred\n",
    "            count += 1\n",
    "        else:\n",
    "            # Nouveau mot\n",
    "            if count > 0:\n",
    "                # Calculer la moyenne des prédictions pour le mot précédent\n",
    "                avg_pred = current_sum / count\n",
    "                word_predictions[current_word] = avg_pred\n",
    "\n",
    "            # Réinitialiser pour le nouveau mot\n",
    "            current_word = word\n",
    "            current_sum = token_pred\n",
    "            count = 1\n",
    "\n",
    "    # Traitement du dernier mot\n",
    "    if count > 0:\n",
    "        avg_pred = current_sum / count\n",
    "        word_predictions[current_word] = avg_pred\n",
    "\n",
    "    return word_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bon: 0.12\n",
      "jour: 0.65\n",
      "Jo: 0.88\n",
      "hn: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "token_predictions = [0.12, 0.65, 0.88, 0.45]\n",
    "words = [\"bon\", \"jour\", \"Jo\", \"hn\"]\n",
    "\n",
    "word_predictions = aggregate_predictions_by_word(token_predictions, words)\n",
    "\n",
    "# Afficher les prédictions par mot\n",
    "for word, pred in word_predictions.items():\n",
    "    print(f\"{word}: {pred}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}